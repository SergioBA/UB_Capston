{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af421117",
   "metadata": {},
   "source": [
    "# Capstone Project: Cápsula Endoscópica\n",
    "**Universidad:** Universitat de Barcelona (UB)\n",
    "**Estudios:** Postgrado de Introducción a la Data Science y al Machine Learning\n",
    "\n",
    "**Integrantes del equipo:**\n",
    " - Javier Sánchez Molino\n",
    " - Sergio Bravo Allué\n",
    " - Marc Bernabé Espinosa\n",
    " - Josep Fontana Castillo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aec519",
   "metadata": {},
   "source": [
    "## 1. Introducción y Objetivo\n",
    "A partir de 44 vídeos de exploraciones endoscópicas de los que se han extraído distintos fotogramas como imágenes, clasificarlas en un total de 14 clases diferentes según el tipo de anomalía o parte del cuerpo presente en la imagen.\n",
    "\n",
    "Los datos consisten en un total de 47.238 imágenes etiquetadas según las 14 clases detalladas a continuación:\n",
    "* A) Angiectasia\n",
    "* B) Blood\n",
    "* C) Erosion\n",
    "* D) Erythematous\n",
    "* E) Foreign Bodies\n",
    "* F) Ileo-cecal valve\n",
    "* G) Lymphoid Hyperplasia\n",
    "* H) Normal mucosa\n",
    "* I) Pylorus\n",
    "* J) Reduced Mucosal View\n",
    "* K) Ulcer\n",
    "* Ampulla of vater\n",
    "* Hematin\n",
    "* Polyp\n",
    "\n",
    "En el *paper* original [Kvasir-Capsule, a video capsule endoscopy dataset](https://osf.io/gr7bn/) se descartan las 3 últimas clases debido a su escasa representación (10, 12 y 64 imágenes, respectivamente). Así pues, el problema de clasificación se centrará en las 11 clases restantes y un total de 47.161 imágenes, con la siguiente distribución:\n",
    "\n",
    "**INTRODUIR HISTOGRAMA DE LES 11 CLASSES**\n",
    "\n",
    "El *pylorus* es el punto de unión del estómago y el intestino delgado, y la *ileocecal valve* marca la transición entre el intestino delgado y el grueso. De las 9 clases restantes, la *normal mucosa* corresponde a imágenes en las que se ve con claridad la mucosa del intestino delgado, mientras que la clase *reduced mucosal view* corresponde a una imagen poco clara o con elementos que dificultan la visión de la mucosa. Las 7 clases restantes sí corresponden a patologías del sistema gastrointestinal.\n",
    "\n",
    "A pesar de la naturaleza distinta que presentan las diferentes clases no se les da un tratamiento diferenciado como problema de clasificación.\n",
    "\n",
    "En el *paper* original se utilizan dos modelos de clasificación. Ambos corresponden a redes neuronales convolucionales (CNN, por sus siglas en inglés) con arquitecturas que han demostrado tener un buen comportamiento en la clasificación de imágenes del sistema gastrointestinal a partir de colonoscopias normales (no de imágenes procedentes de una cápsula endoscópica ingerida):\n",
    "\n",
    "* ***ResNet-152***: arquitectura ganadora de la *ImageNet Challenge* (ILSVRC) de 2015 formada por una CNN con 152 capas que presentaba como principal innovación el uso de *skip connections*, es decir, la unión del input de una capa al output de otra capa varios niveles por encima.\n",
    "    **AFEGIR REFERENCIA**\n",
    "* ***DenseNet-161***: ***COMPLETAR AMB UNA BREU DESCRIPCIÓ DE L'ARQUITECTURA***\n",
    "\n",
    "El objetivo del presente proyecto es replicar los resultados obtenidos en la investigación original, aplicar distintas variaciones y tratamientos para determinar si mejoran los resultados, y ampliar el estudio a otros modelos y estrategias que permitan obtener una mejor clasificación de las imágenes según distintas métricas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89051e28",
   "metadata": {},
   "source": [
    "## 2. Metodología\n",
    "### 2.1. Datos\n",
    "Se ha descargado el *dataset* original del [link](https://osf.io/dv2ag/) especificado en el *paper*. Los autores dividieron los datos en dos subgrupos (*split 0* y *split 1*) e hicieron el doble ejercicio de aplicar ambos modelos tomando el primer subgrupo de datos para entrenar, y el segundo para validar, y viceversa.\n",
    "\n",
    "El *split 0* presenta la siguiente distribución por clases:\n",
    "\n",
    "***INCLOURE HISTOGRAMA DEL SPLIT 0***\n",
    "\n",
    "El *split 1*, en cambio, presenta la siguiente distribución por clases:\n",
    "\n",
    "***INCLOURE HISTOGRAMA DEL SPLIT 1***\n",
    "\n",
    "Vemos que...***COMPLETAR!!!***\n",
    "\n",
    "Una de las variaciones que consideraremos más adelante (ver apartado ***COMPLETAR REFERÈNCIA***) será realizar otras divisiones de los datos para ver si afecta de manera significativa al resultado final.\n",
    "\n",
    "\n",
    "### 2.2. Código\n",
    "El [script](https://github.com/simula/kvasir-capsule) original está desarrollado en Python y utiliza ***PyTorch*** como librería principal de Deep Learning. Sin embargo, hemos preferido hacer uso de [Keras](https://keras.io/) para el presente proyecto. ***Keras*** es una API de Deep Learning de alto nivel construida sobre [TensorFlow 2](https://www.tensorflow.org/) y su extensivo uso en toda la comunidad de usuarios y desarrolladores de Deep Learning nos ha permitido disponer de un gran número de recursos y ayudas. Además, resulta idónea para introducirse en esta disciplina y poder utilizar los modelos con mejor comportamiento sin necesidad de conocimientos previos especializados ni una comprensión profunda de su implementación.\n",
    "\n",
    "\n",
    "### 2.3. Entorno de ejecución\n",
    "La manipulación de todas las imágenes así como el entrenamiento de los modelos ha resultado ser muy exigente en cuanto a capacidad de memoria RAM requerida, así como potencia de cálculo. Así pues, aquellos componentes del equipo sin disponibilidad de un ordenador potente con GPU han tenido que realizar las ejecuciones en Google Collab.\n",
    "\n",
    "El resultado de las ejecuciones incluídas en este documento se han obtenido con una máquina ***COMPLETAR AMB LES CARACTERÍSTIQUES DE LA MÀQUINA***.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849bbfa2",
   "metadata": {},
   "source": [
    "## 3. Estudios a realizar\n",
    "Se presentan numerosas alternativas a probar y contrastar que además pueden combinarse entre ellas de distintas formas. A continuación se enumeran y explican las que hemos decidido estudiar.\n",
    "\n",
    "### 3.1. Splits de los datos\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 3.2. Preprocesado y Data Augmentation\n",
    "Después de [cargar](https://www.tensorflow.org/tutorials/load_data/images?hl=en) todas las imágenes mediante las utilidades específicas de Keras, el **preprocesado** básico a aplicar consiste en reescalar las imágenes para que los valores de los píxels no esté comprendido en el rango usual de [0,255] sino que cubra el intervalo de valores [-1,1] o bien [0,1].\n",
    "\n",
    "El método de **Data Augmentation** consiste en incrementar artificialmente el número de imágenes del subrgrupo de entrenamiento mediante la generación de variantes realistas de las imágenes originales. \n",
    "\n",
    "Las técnicas para generar nuevas imágenes consisten en aplicar de forma aleatoria giros, simetrías, modificaciones del contraste o zooms sobre determinadas zonas de la imagen original. Ésta última opción la hemos descartado por considerar que podría llevar a equívoco al modelo si justo se hace zoom sobre una zona en la que no se encuentra la anomalía correspondiente, puesto que la imagen resultante siempre se etiqueta igual que la de partida. En el *script* original tampoco aplican zooms, pero sí el resto de técnicas. \n",
    "\n",
    "Existe la opción de realizar la *data augmentation* como parte del preprocesado, antes de enviar las imágenes al modelo. Sin embargo, también pueden incorporarse [capas iniciales extras](https://www.tensorflow.org/tutorials/images/data_augmentation?hl=en) en la CNN que se encarguen de realizar este incremento de imágenes y que sólo aplicará cuando se trate del subconjunto de entrenamiento. Nosotros hemos optado por esta segunda opción.\n",
    "\n",
    "***EXEMPLE DE CODI DE DATA AUGMENTATION***\n",
    "\n",
    "***INCLOURE DOS GRÀFIQUES: UNA AMB DA I L'ALTRE SENSE, QUE PERMETIN CONTRASTAR LA DIFERÈNCIA***\n",
    "\n",
    "\n",
    "\n",
    "### 3.3. Arquitecturas de modelos CNN - Transfer Learning\n",
    "\n",
    "\n",
    "De las dos arquitecturas distintas de CNN que se utilizan en el *paper* original la **DenseNet-161** no está disponible en Keras, sólo en PyTorch. Sin embargo, después de explorar las distintas opciones de arquitecturas que nos ofrece Keras, hemos decidido sustituirla por una de parecida, la *DenseNet-169*.\n",
    "\n",
    "Así pues, hemos utilizado tres arquitecturas de CNN distintas:\n",
    "\n",
    "1. ***ResNet-152***\n",
    "2. ***DenseNet-169***\n",
    "3. ***EficientNet B1***: \n",
    "\n",
    "A continuación se muestra una gráfica comparativa entre distintos modelos según el número de parámetros utilizados y el nivel de *Top 1 Accuracy* sobre la base de imágenes de ImageNet:\n",
    "\n",
    "![ModelsComparison](https://www.researchgate.net/publication/352346653/figure/fig5/AS:1033729019486210@1623471612282/Model-Size-vs-Accuracy-Comparison-EfficientNet-B0-is-the-baseline-network-developed-by.jpg)\n",
    "\n",
    "\n",
    "### 3.4. Hiperparámetros de los modelos - Datos desbalanceados\n",
    "\n",
    "\n",
    "### 3.5. Batch size\n",
    "\n",
    "\n",
    "### 3.6. Métricas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a24f66",
   "metadata": {},
   "source": [
    "1. Primera enumeracio\n",
    "2. Segona enumeracio\n",
    "\n",
    "\n",
    "**Negreta** i _Cursiva_ i `Codi` de text\n",
    "\n",
    "[link](url) i ![Image](src)\n",
    "\n",
    "$x_i$ element of a vector, $\\textbf{x}$ column vector, $\\textbf{x'}$ (transpose of $\\textbf{x}$) row vector, $X$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9561f4de",
   "metadata": {},
   "source": [
    "<img style=\"border-radius:40px;\" src=\"./images/Desnet_WithAug_dataSetRandomSplits_30Percent_ConfMatrix.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a19037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
