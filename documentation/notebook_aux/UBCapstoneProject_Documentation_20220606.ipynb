{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af421117",
   "metadata": {},
   "source": [
    "# Capstone Project: Cápsula Endoscópica\n",
    "**Universidad:** Universitat de Barcelona (UB)\n",
    "**Estudios:** Postgrado de Introducción a la Data Science y al Machine Learning\n",
    "\n",
    "**Integrantes del equipo:**\n",
    " - Javier Sánchez Molino\n",
    " - Sergio Bravo Allué\n",
    " - Marc Bernabé Espinosa\n",
    " - Josep Fontana Castillo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aec519",
   "metadata": {},
   "source": [
    "## 1. Introducción y Objetivo\n",
    "A partir de 44 vídeos de exploraciones endoscópicas de los que se han extraído distintos fotogramas como imágenes, clasificarlas en un total de 14 clases diferentes según el tipo de anomalía o parte del cuerpo presente en la imagen.\n",
    "\n",
    "Los datos consisten en un total de 47.238 imágenes etiquetadas según las 14 clases detalladas a continuación:\n",
    "* A) Angiectasia\n",
    "* B) Blood\n",
    "* C) Erosion\n",
    "* D) Erythematous\n",
    "* E) Foreign Bodies\n",
    "* F) Ileo-cecal valve\n",
    "* G) Lymphoid Hyperplasia\n",
    "* H) Normal mucosa\n",
    "* I) Pylorus\n",
    "* J) Reduced Mucosal View\n",
    "* K) Ulcer\n",
    "* Ampulla of vater\n",
    "* Hematin\n",
    "* Polyp\n",
    "\n",
    "En el *paper* original [Kvasir-Capsule, a video capsule endoscopy dataset](https://osf.io/gr7bn/) se descartan las 3 últimas clases debido a su escasa representación (10, 12 y 64 imágenes, respectivamente). Así pues, el problema de clasificación se centrará en las 11 clases restantes y un total de 47.161 imágenes, con la siguiente distribución:\n",
    "\n",
    "\n",
    "<font color='red'> **INTRODUIR HISTOGRAMA DE LES 11 CLASSES**\n",
    "\n",
    "    \n",
    "<font color='black'> El *pylorus* es el punto de unión del estómago y el intestino delgado, y la *ileocecal valve* marca la transición entre el intestino delgado y el grueso. De las 9 clases restantes, la *normal mucosa* corresponde a imágenes en las que se ve con claridad la mucosa del intestino delgado, mientras que la clase *reduced mucosal view* corresponde a una imagen poco clara o con elementos que dificultan la visión de la mucosa. Las 7 clases restantes sí corresponden a patologías del sistema gastrointestinal.\n",
    "\n",
    "A pesar de la naturaleza distinta que presentan las diferentes clases no se les da un tratamiento diferenciado como problema de clasificación.\n",
    "\n",
    "En el *paper* original se utilizan dos modelos de clasificación. Ambos corresponden a redes neuronales convolucionales (CNN, por sus siglas en inglés) con arquitecturas que han demostrado tener un buen comportamiento en la clasificación de imágenes del sistema gastrointestinal a partir de colonoscopias normales (no de imágenes procedentes de una cápsula endoscópica ingerida):\n",
    "\n",
    "* ***ResNet-152***: arquitectura ganadora de la *ImageNet Challenge* (ILSVRC) de 2015 formada por una CNN con 152 capas que presentaba como principal innovación el uso de *skip connections*, es decir, la unión del input de una capa al output de otra capa varios niveles por encima.\n",
    "    <font color='red'>**AFEGIR REFERENCIA AURÉLIEN**<font color='black'>\n",
    "* ***DenseNet-161***: se caracteriza por presentar conexiones densas entre las distintas capas mediante los llamados [*dense blocks*](https://paperswithcode.com/method/dense-block), en los que las capas que lo componen están todas conectadas entre sí directamente unas con otras. Para mantener la naturaleza de retroalimentación de las CNN, cada capa dentro de un *dense block* obtiene inputs adicionales de todas las capas precedentes, y pasa su propio *feature-map* a las capas siguientes.\n",
    "\n",
    "El objetivo del presente proyecto es replicar los resultados obtenidos en la investigación original, aplicar distintas variaciones y tratamientos para determinar si mejoran los resultados, y ampliar el estudio a otros modelos y estrategias que permitan obtener una mejor clasificación de las imágenes según distintas métricas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89051e28",
   "metadata": {},
   "source": [
    "## 2. Metodología\n",
    "### 2.1. Datos\n",
    "Se ha descargado el *dataset* original del [link](https://osf.io/dv2ag/) especificado en el *paper*. Los autores dividieron los datos en dos subgrupos (*split 0* y *split 1*) e hicieron el doble ejercicio de aplicar ambos modelos tomando el primer subgrupo de datos para entrenar, y el segundo para validar, y viceversa.\n",
    "\n",
    "El *split 0* presenta la siguiente distribución por clases:\n",
    "\n",
    "<font color='red'>***INCLOURE HISTOGRAMA DEL SPLIT 0***\n",
    "\n",
    "<font color='black'>El *split 1*, en cambio, presenta la siguiente distribución por clases:\n",
    "\n",
    "<font color='red'>***INCLOURE HISTOGRAMA DEL SPLIT 1***\n",
    "\n",
    "<font color='black'>Vemos que...<font color='red'>***COMPLETAR AMB COMENTARIS SOBRE SI LA DISTRIBUCIÓ ENTRE CLASSES ÉS SIMILAR O NO***\n",
    "\n",
    "<font color='black'>Una de las variaciones que consideraremos más adelante (ver apartado 3.1. <font color='red'>***¿POT CREAR-SE REFERÈNCIA DINÀMICA?***<font color='black'>) será realizar otras divisiones de los datos para ver si afecta de manera significativa al resultado final.\n",
    "\n",
    "\n",
    "### 2.2. Código\n",
    "El [script](https://github.com/simula/kvasir-capsule) original está desarrollado en Python y utiliza ***PyTorch*** como librería principal de Deep Learning. Sin embargo, hemos preferido hacer uso de [Keras](https://keras.io/) para el presente proyecto. ***Keras*** es una API de Deep Learning de alto nivel construida sobre [TensorFlow 2](https://www.tensorflow.org/) y su extensivo uso en toda la comunidad de usuarios y desarrolladores de Deep Learning nos ha permitido disponer de un gran número de recursos y ayudas. Además, resulta idónea para introducirse en esta disciplina y poder utilizar los modelos con mejor comportamiento sin necesidad de conocimientos previos especializados ni una comprensión profunda de su implementación.\n",
    "\n",
    "\n",
    "### 2.3. Entorno de ejecución\n",
    "La manipulación de todas las imágenes así como el entrenamiento de los modelos ha resultado ser muy exigente en cuanto a capacidad de memoria RAM requerida, así como potencia de cálculo. Así pues, aquellos componentes del equipo sin disponibilidad de un ordenador potente con GPU han tenido que realizar las ejecuciones en Google Collab.\n",
    "\n",
    "El resultado de las ejecuciones incluídas en este documento se han obtenido con una máquina <font color='red'>***COMPLETAR AMB LES CARACTERÍSTIQUES DE LA MÀQUINA***.\n",
    "\n",
    "<font color='black'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849bbfa2",
   "metadata": {},
   "source": [
    "## 3. Estudios a realizar\n",
    "Se presentan numerosas alternativas a probar y contrastar que además pueden combinarse entre ellas de distintas formas. A continuación se enumeran y explican las que hemos decidido estudiar.\n",
    "\n",
    "### 3.1. Splits de los datos\n",
    "Además de la subdivisión en dos grupos utilizada en el *paper* original, también hemos decidido probar dos *splits* aleatorios extras, uno con el 30% de los datos para validación y el otro con el 50%. En ambos casos, la división se ha realizado de forma **estratificada**, es decir, manteniendo la proporción original de imágenes de las distintas clases entre el subgrupo de entrenamiento y el de validación.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 3.2. Preprocesado y Data Augmentation\n",
    "Después de [cargar](https://www.tensorflow.org/tutorials/load_data/images?hl=en) todas las imágenes mediante las utilidades específicas de Keras, el **preprocesado** básico a aplicar consiste en reescalar las imágenes para que los valores de los píxels no esté comprendido en el rango usual de [0,255] sino que cubra el intervalo de valores [-1,1] o bien [0,1].\n",
    "\n",
    "El método de **Data Augmentation** consiste en incrementar artificialmente el número de imágenes del subrgrupo de entrenamiento mediante la generación de variantes realistas de las imágenes originales. De este modo se reduce el *overfitting* del modelo y funciona como técnica de **regularización**. <font color='red'>***INCLOURE REFERÈNCIA A LA PAG. 613 DE L'AURELIEN***\n",
    "\n",
    "<font color='black'> Las técnicas para generar nuevas imágenes consisten en aplicar de forma aleatoria giros, simetrías, modificaciones del contraste o zooms sobre determinadas zonas de la imagen original. Ésta última opción la hemos descartado por considerar que podría llevar a equívoco al modelo si justo se hace zoom sobre una zona en la que no se encuentra la anomalía correspondiente, puesto que la imagen resultante siempre se etiqueta igual que la de partida. En el *script* original tampoco aplican zooms, pero sí el resto de técnicas. \n",
    "\n",
    "Existe la opción de realizar la *data augmentation* como parte del preprocesado, antes de enviar las imágenes al modelo. Sin embargo, también pueden incorporarse [capas iniciales extras](https://www.tensorflow.org/tutorials/images/data_augmentation?hl=en) en la CNN que se encarguen de realizar este incremento de imágenes y que sólo aplicará cuando se trate del subconjunto de entrenamiento. Nosotros hemos optado por esta segunda opción.\n",
    "\n",
    "<font color='red'>***EXEMPLE DE CODI DE DATA AUGMENTATION***\n",
    "\n",
    "***INCLOURE DOS GRÀFIQUES: UNA AMB DA I L'ALTRE SENSE, QUE PERMETIN CONTRASTAR LA DIFERÈNCIA***\n",
    "\n",
    "<font color='black'>\n",
    "\n",
    "\n",
    "### 3.3. Arquitecturas de modelos CNN - Transfer Learning\n",
    "\n",
    "\n",
    "De las dos arquitecturas distintas de CNN que se utilizan en el *paper* original, la **DenseNet-161** no está disponible en Keras, sólo en PyTorch. Sin embargo, después de explorar las distintas opciones de arquitecturas que nos ofrece Keras, hemos decidido sustituirla por una de parecida, la *DenseNet-169*.\n",
    "\n",
    "Así pues, hemos utilizado tres arquitecturas de CNN distintas:\n",
    "\n",
    "1. ***ResNet-152***\n",
    "    \n",
    "2. ***DenseNet-169***: tiene un nivel de *accuracy* sobre ImageNet ligeramente [inferior](https://paperswithcode.com/model/densenet?variant=densenet-161) a la *DenseNet-161*, pero las consideramos suficientemente asimilables.\n",
    "    \n",
    "3. ***EfficientNet-B1***: la familia de redes *EfficientNet* fueron introducidas por primera vez en el paper \"[EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)\" de 2019, y se caracterizan por presentar un equilibrio excelente entre número de parámetros y nivel de acierto en la clasificación respecto a otras redes ampliamente utilizadas, tal y como puede verse en el gráfico de más abajo. Además, la construcción de las distintas redes de la familia se basa en el modelo base *B0* que se reescala de forma uniforme para cada dimensión de la CNN (anchura, profundidad y resolución). <font color='red'> ***AFEGIR REFERÈNCIA (https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html)***\n",
    "\n",
    "A continuación se muestra una gráfica comparativa entre distintos modelos según el número de parámetros utilizados y el nivel de *Top-1 Accuracy* sobre la base de imágenes de ImageNet:\n",
    "\n",
    "![ModelsComparison](https://www.researchgate.net/publication/352346653/figure/fig5/AS:1033729019486210@1623471612282/Model-Size-vs-Accuracy-Comparison-EfficientNet-B0-is-the-baseline-network-developed-by.jpg)\n",
    "\n",
    "Se ha decidido utilizar la variante *EfficientNet-B1* por presentar un número reducido de parámetros y una *Top-1 accuracy* superior a la de la *ResNet-152* y *DenseNet-201*.\n",
    "    \n",
    "Para los tres modelos se ha seguido una metodología de **transfer learning** estricta en la que se han mantenido los pesos originales de los modelos entrenados con la base de imágenes de ImageNet, quitándoles sólo la última capa, correspondiente al clasificador final.\n",
    "    \n",
    "    \n",
    "<font color='red'>***EXPLICAR LES CAPES EXTRES QUE HEM AFEGIT***\n",
    "    \n",
    "<font color='red'>***SI AL FINAL FEM PROVES REENTRENANT UNA PART DELS MODELS, EXPLICAR-HO***\n",
    "    \n",
    "\n",
    "<font color='black'>\n",
    "    \n",
    "    \n",
    "### 3.4. Hiperparámetros de los modelos - Datos desbalanceados\n",
    "\n",
    "<font color='red'>***AQUÍ EXPLICAREM TEMA PESOS PER CLASSE I COM ELS HEM CALCULAT***\n",
    "\n",
    "<font color='black'>\n",
    "    \n",
    "### 3.5. Batch size\n",
    "    \n",
    "<font color='red'>***EL QUE VAM DISCUTIR DIMARTS PASSAT. INCLOURE ARTICLE QUE VA TROBAR EL MARC***\n",
    "\n",
    "    (https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network)\n",
    "    \n",
    "<font color='black'>\n",
    "\n",
    "\n",
    "### 3.6. Métricas\n",
    "\n",
    "<font color='red'>***TOT I QUE NO AFECTA AL RESULTAT, SÍ A LA MANERA D'INTERPRETAR-LO, I CREC QUE VAL LA PENA INCLOURE L'APARTAT. AQUÍ ES TRACTARÀ DE DIR QUE S'HAN UTILITZAT LES MÈTRIQUES QUE JA TENIA EL SKLEARN, SENSE NECESSITAT D'HAVER-LES HAGUT DE DEFINIR NOSALTRES MANUALMENT***\n",
    "\n",
    "<font color='black'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c24533",
   "metadata": {},
   "source": [
    "### XULETES MARKDOWN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c71e34",
   "metadata": {},
   "source": [
    "$x_i$ element of a vector, $\\textbf{x}$ column vector, $\\textbf{x'}$ (transpose of $\\textbf{x}$) row vector, $X$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9561f4de",
   "metadata": {},
   "source": [
    "Com incloure al document una imatge guardada en una carpeta a la mateixa ruta que el notebook:\n",
    "\n",
    "<img style=\"border-radius:40px;\" src=\"./images/03/Desnet_WithAug_dataSetRandomSplits_30Percent_ConfMatrix.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c857b35b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
