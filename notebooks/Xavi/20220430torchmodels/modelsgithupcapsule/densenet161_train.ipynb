{"cells": [{"cell_type": "markdown", "metadata": {}, "source": [" # Developer: Vajira Thambawita<br>\n", " # Last modified date: 18/07/2018<br>\n", " # ##################################"]}, {"cell_type": "markdown", "metadata": {}, "source": [" # Description ##################<br>\n", " # pythroch resnet18 training"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#########################################"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from __future__ import print_function, division"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import datetime"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#start = datetime.datetime.now()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import argparse\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import torch.optim as optim\n", "from torch.optim import lr_scheduler\n", "from torchvision import datasets, models, transforms, utils\n", "import pickle\n", "#from pandas_ml import ConfusionMatrix\n", "import matplotlib as mpl\n", "import matplotlib.pyplot as plt\n", "import time\n", "import os\n", "import copy\n", "import sys\n", "import yaml\n", "import pandas as pd\n", "import numpy as np"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import sklearn.metrics as mtc\n", "from sklearn.metrics import confusion_matrix\n", "from sklearn.metrics import classification_report\n", "import itertools\n", "from multiprocessing import Process, freeze_support\n", "from torch.utils.tensorboard import SummaryWriter"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from tqdm import tqdm\n", "from torchsummary import summary\n", "from torch.autograd import Variable"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from dataset.Dataloader_with_path import ImageFolderWithPaths as dataset"]}, {"cell_type": "markdown", "metadata": {}, "source": ["=====================================<br>\n", "Get and set all input parameters<br>\n", "====================================="]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["parser = argparse.ArgumentParser()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Hardware"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["parser.add_argument(\"--device\", default=\"gpu\", help=\"Device to run the code\")\n", "parser.add_argument(\"--device_id\", type=int, default=0, help=\"\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["parser.add_argument(\"--py_file\",default=os.path.abspath(__file__)) # store current python file"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Directories"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["parser.add_argument(\"--data_train_folder\", \n", "                default=\"/work/vajira/DATA/kvasir_capsule/data/new_splits/split_0\",\n", "                help=\"Train data folder\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["parser.add_argument(\"--data_val_folder\", \n", "                default=\"/work/vajira/DATA/kvasir_capsule/data/new_splits/split_1\",\n", "                help=\"Validation data folder\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["parser.add_argument(\"--out_dir\", \n", "                default=\"/work/vajira/DATA/kvasir_capsule/output\",\n", "                help=\"Main output dierectory\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["parser.add_argument(\"--tensorboard_dir\", \n", "                default=\"/work/vajira/DATA/kvasir_capsule/tensorboard\",\n", "                help=\"Folder to save output of tensorboard\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Hyper parameters"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["parser.add_argument(\"--bs\", type=int, default=32, help=\"Mini batch size\")\n", "parser.add_argument(\"--lr\", type=float, default=0.001, help=\"Learning rate for training\")\n", "parser.add_argument(\"--num_workers\", type=int, default=16, help=\"Number of workers in dataloader\")\n", "parser.add_argument(\"--weight_decay\", type=float, default=1e-5, help=\"weight decay of the optimizer\")\n", "parser.add_argument(\"--momentum\", type=float, default=0.9, help=\"Momentum of SGD function\")\n", "parser.add_argument(\"--lr_sch_factor\", type=float, default=0.1, help=\"Factor to reduce lr in the scheduler\")\n", "parser.add_argument(\"--lr_sch_patience\", type=int, default=10, help=\"Num of epochs to be patience for updating lr\")\n", "parser.add_argument(\"--lr_to_stop\", type=float, default=0.00001, help=\"Num of epochs to be patience for updating lr\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Action handling "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["parser.add_argument(\"--num_epochs\", type=int, default=2000, help=\"Numbe of epochs to train\")\n", "# parser.add_argument(\"--start_epoch\", type=int, default=0, help=\"Start epoch in retraining\")\n", "parser.add_argument(\"action\", type=str, help=\"Select an action to run\", choices=[\"train\", \"retrain\", \"test\", \"check\", \"prepare\"])\n", "parser.add_argument(\"--checkpoint_interval\", type=int, default=25, help=\"Interval to save checkpoint models\")\n", "#parser.add_argument(\"--val_fold\", type=str, default=\"0\", help=\"Select the validation fold\", choices=[\"fold_1\", \"fold_2\", \"fold_3\"])\n", "#parser.add_argument(\"--all_folds\", default=[\"0\", \"1\"], help=\"list of all folds available in data folder\")\n", "parser.add_argument(\"--test_checkpoint\", help=\"Checkpoint to test or generate results\")\n", "parser.add_argument(\"--weights\", default=[0.0285, 1.0000, 0.1068, 0.1667, 0.0373, 0.0196, 0.0982, 0.0014, 0.0235, 0.0236, 0.0809], help=\"Weights for class\")\n", "opt = parser.parse_args()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["=========================================<br>\n", "Device handling<br>\n", "========================================="]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["torch.cuda.set_device(opt.device_id)\n", "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["==========================================<br>\n", "Folder handling<br>\n", "=========================================="]}, {"cell_type": "markdown", "metadata": {}, "source": ["ake output folder if not exist"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["os.makedirs(opt.out_dir, exist_ok=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["make subfolder in the output folder "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["py_file_name = opt.py_file.split(\"/\")[-1] # Get python file name (soruce code name)\n", "checkpoint_dir = os.path.join(opt.out_dir, py_file_name + \"/checkpoints\")\n", "os.makedirs(checkpoint_dir, exist_ok=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["make tensorboard subdirectory for the experiment"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tensorboard_exp_dir = os.path.join(opt.tensorboard_dir, py_file_name)\n", "os.makedirs( tensorboard_exp_dir, exist_ok=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["=========================================<br>\n", "Tensorboard<br>\n", "=========================================<br>\n", "Initialize summary writer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["writer = SummaryWriter(tensorboard_exp_dir)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#########################################################<br>\n", "#########################################################<br>\n", "#########################################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["=========================================<br>\n", "Prepare Data<br>\n", "========================================="]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def prepare_data():\n", "    data_transforms = {\n", "        'train': transforms.Compose([\n", "            transforms.Resize(256),\n", "            transforms.CenterCrop(256),\n", "            transforms.Resize(224),\n", "            transforms.RandomHorizontalFlip(),\n", "            transforms.RandomVerticalFlip(),\n", "            transforms.RandomRotation(90),\n", "            transforms.ToTensor(),\n", "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n", "        ]),\n", "        'validation': transforms.Compose([\n", "            transforms.Resize(256),\n", "            transforms.CenterCrop(256),\n", "            transforms.Resize(224),\n", "            transforms.ToTensor(),\n", "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n", "        ]),\n", "    }\n\n", "    # Train dataset\n", "    dataset_train = dataset(opt.data_train_folder, data_transforms[\"train\"])\n\n", "    # Validation dataset\n", "    dataset_val = dataset(opt.data_val_folder, data_transforms[\"validation\"])\n", "                                                \n", "    dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=opt.bs,\n", "                                                    shuffle=True, num_workers=opt.num_workers)\n", "    dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=opt.bs,\n", "                                                    shuffle=False, num_workers=opt.num_workers)\n", "                    \n", "    train_size = len(dataset_train)\n", "    val_size = len(dataset_val)\n", "    print(\"train dataset size =\", train_size)\n", "    print(\"validation dataset size=\", val_size)\n", "    print(\"dataset train class order= \", dataset_train.class_to_idx)\n", "    print(\"dataset val class order= \", dataset_train.class_to_idx)\n\n", "    #exit() # just for testing"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["   \n", "    return {\"train\":dataloader_train, \"val\":dataloader_val, \"dataset_size\":{\"train\": train_size, \"val\":val_size} }"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#######################################################################<br>\n", " Printing images just for testing<br>\n", "#######################################################################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "def imshow(img):<br>\n", "    img = img / 2 + 0.5     # unnormalize<br>\n", "    npimg = img.numpy()<br>\n", "    plt.imshow(np.transpose(npimg, (1, 2, 0)))<br>\n", "dataiter = iter(dataloaders['train'])<br>\n", "sample_images, sample_labels = dataiter.next()<br>\n", "npimg = sample_images[0].numpy()<br>\n", "npimg = np.transpose(npimg,(1,2,0))<br>\n", "plt.imshow(npimg[:,:, 0])<br>\n", "plt.show()<br>\n", "print(npimg[:, :, 0])<br>\n", "#imshow(utils.make_grid(sample_images))<br>\n", "input()<br>\n", "exit()<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["=========================================================<br>\n", "Train model<br>\n", "=========================================================="]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train_model(model, optimizer, criterion, dataloaders: dict, scheduler, best_acc=0.0, start_epoch = 0):\n", "    best_model_wts = copy.deepcopy(model.state_dict())\n", "    \n", "    for epoch in range(start_epoch , start_epoch + opt.num_epochs ):\n", "        for phase in [\"train\", \"val\"]:\n", "            if phase == \"train\":\n", "                model.train()\n", "                dataloader = dataloaders[\"train\"]\n", "            else:\n", "                model.eval()\n", "                dataloader = dataloaders[\"val\"]\n", "            \n", "            \n", "            running_loss = 0.0\n", "            running_corrects = 0"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["            for i, data in enumerate(dataloader, 0):\n", "                inputs, labels, paths = data\n", "                inputs = inputs.to(device)\n", "                labels = labels.to(device)\n", "                # zero the parameter gradients\n", "                optimizer.zero_grad()\n", "                # forward\n", "                # track history if only in train\n", "                with torch.set_grad_enabled(phase == 'train'):\n", "                    outputs = model(inputs)\n", "                    _, preds = torch.max(outputs, 1)\n", "                    loss = criterion(outputs, labels)\n", "                    #  print(\"outputs=\", outputs) # only for testing - vajira\n", "                    #  print(\"labels = \", labels) # only for testing - vajira\n", "                    # backward + optimize only if in training phase\n", "                    if phase == 'train':\n", "                        loss.backward()\n", "                        optimizer.step()\n", "                # statistics\n", "                running_loss += loss.item() * inputs.size(0)\n", "                running_corrects += torch.sum(preds == labels.data)\n", "            epoch_loss = running_loss / dataloaders[\"dataset_size\"][phase]\n", "            epoch_acc = running_corrects.double() / dataloaders[\"dataset_size\"][phase]\n\n", "            # update tensorboard writer\n", "            writer.add_scalars(\"Loss\", {phase:epoch_loss}, epoch)\n", "            writer.add_scalars(\"Accuracy\" , {phase:epoch_acc}, epoch)\n", "             # update the lr based on the epoch loss\n", "            if phase == \"val\": \n", "                # keep best model weights\n", "                if epoch_acc > best_acc:\n", "                    best_acc = epoch_acc\n", "                    best_model_wts = copy.deepcopy(model.state_dict())\n", "                    best_epoch =epoch\n", "                    best_epoch_loss = epoch_loss\n", "                    best_epoch_acc = epoch_acc\n", "                    print(\"Found a better model\")\n", "                # Get current lr\n", "                lr = optimizer.param_groups[0]['lr']\n", "                #print(\"lr=\", lr)\n", "                writer.add_scalar(\"LR\", lr, epoch)\n", "                scheduler.step(epoch_loss) \n", "                # Early stop if lr is too small\n", "                if lr <= opt.lr_to_stop:\n", "                    print(\"LR reached to :\", lr)\n", "                    save_model(best_model_wts, best_epoch, best_epoch_loss, best_epoch_acc)\n", "                    print(\"Best model saved\")\n", "                    # print(\"LR reached to :\", current_lr)\n", "                    print(\"Model exits\")\n", "                    return \n", "            "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["            # Print output\n", "            print('Epoch:\\t  %d |Phase: \\t %s | Loss:\\t\\t %.4f | Acc:\\t %.4f '\n", "                      % (epoch, phase, epoch_loss, epoch_acc))\n", "    \n", "    save_model(best_model_wts, best_epoch, best_epoch_loss, best_epoch_acc)\n", "            \n", "#===============================================\n", "# Prepare models\n", "#==============================================="]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def prepare_model():\n", "    model = models.densenet161(pretrained=True)\n", "    num_ftrs = model.classifier.in_features\n", "    model.classifier = nn.Linear(num_ftrs, 11)\n", "    model = model.to(device)\n", "    \n", "    return model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["===================================<br>\n", "Run training process<br>\n", "==================================="]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def run_train(retrain=False):\n", "    model = prepare_model()\n", "    \n", "    dataloaders = prepare_data()\n\n", "    # optimizer = optim.Adam(model.parameters(), lr=opt.lr , weight_decay=opt.weight_decay)\n", "    optimizer = optim.SGD(model.parameters(), lr=opt.lr, momentum=0.9)\n", "    # optimizer = optim.SGD(model.parameters(), lr=opt.lr )\n\n", "    # criterion =  nn.MSELoss() # backprop loss calculation\n", "    weight_tensor = torch.FloatTensor(opt.weights).to(device)\n", "    criterion = nn.CrossEntropyLoss(weight=weight_tensor) # weight=weights\n", "    # criterion_validation = nn.L1Loss() # Absolute error for real loss calculations\n\n", "    # LR shceduler\n", "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=opt.lr_sch_factor, patience=opt.lr_sch_patience, verbose=True)\n\n", "    # call main train loop\n", "    if retrain:\n", "        # train from a checkpoint\n", "        checkpoint_path = input(\"Please enter the checkpoint path:\")\n", "        checkpoint = torch.load(checkpoint_path)\n", "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n", "        start_epoch = checkpoint[\"epoch\"]\n", "        loss = checkpoint[\"loss\"]\n", "        acc = checkpoint[\"acc\"]\n", "        train_model(model,optimizer,criterion, dataloaders, scheduler, best_acc=acc, start_epoch=start_epoch)\n", "    else:\n", "        train_model(model,optimizer,criterion, dataloaders, scheduler, best_acc=0.0, start_epoch=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["====================================<br>\n", "Save models<br>\n", "===================================="]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def save_model(model_weights,  best_epoch,  best_epoch_loss, best_epoch_acc):\n", "   \n", "    check_point_name = py_file_name + \"_epoch:{}_acc:{}.pt\".format(best_epoch, best_epoch_acc) # get code file name and make a name\n", "    check_point_path = os.path.join(checkpoint_dir, check_point_name)\n", "    # save torch model\n", "    torch.save({\n", "        \"epoch\": best_epoch,\n", "        \"model_state_dict\": model_weights,\n", "        # \"optimizer_state_dict\": optimizer.state_dict(),\n", "        # \"train_loss\": train_loss,\n", "        \"loss\": best_epoch_loss,\n", "        \"acc\": best_epoch_acc,\n", "    }, check_point_path)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["====================================<br>\n", "Check model<br>\n", "===================================="]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def check_model_graph():\n", "    model = prepare_model()\n", "    summary(model, (3, 224, 224)) # this run on GPU\n", "    model = model.to('cpu')\n", "    #dataloaders = prepare_data()\n", "    #sample = next(iter(dataloaders[\"train\"]))\n\n", "    #inputs = sample[\"features\"]\n", "   # inputs = inputs.to(device, torch.float)\n", "    #print(inputs.shape)\n", "    print(model)\n", "    dummy_input = Variable(torch.rand(13, 3, 224, 224))\n", "    \n", "    writer.add_graph(model, dummy_input) # this need the model on CPU"]}, {"cell_type": "markdown", "metadata": {}, "source": ["==============================================<br>\n", " Model testing method<br>\n", "=============================================="]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def test_model():\n", "    \n", "    test_model_checkpoint = opt.test_checkpoint #input(\"Please enter the path of test model:\")\n", "    checkpoint = torch.load(test_model_checkpoint)\n", "    model = prepare_model()\n", "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n", "    model.eval()\n", "    dataloaders = prepare_data()\n", "    test_dataloader = dataloaders[\"val\"]\n\n", "    # TO collect data\n", "    correct = 0\n", "    total = 0\n", "    all_labels_d = torch.tensor([], dtype=torch.long).to(device)\n", "    all_predictions_d = torch.tensor([], dtype=torch.long).to(device)\n", "    all_predictions_probabilities_d = torch.tensor([], dtype=torch.float).to(device)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    with torch.no_grad():\n", "        for i, data in enumerate(test_dataloader, 0):\n", "            inputs, labels, paths = data\n", "            # print(labels)\n", "            inputs = inputs.to(device)\n", "            labels = labels.to(device)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["            outputs = model(inputs)\n", "            outputs = F.softmax(outputs, 1)\n", "            predicted_probability, predicted = torch.max(outputs.data, 1)\n", "            total += labels.size(0)\n", "            correct += (predicted == labels).sum()\n", "            all_labels_d = torch.cat((all_labels_d, labels), 0)\n", "            all_predictions_d = torch.cat((all_predictions_d, predicted), 0)\n", "            all_predictions_probabilities_d = torch.cat((all_predictions_probabilities_d, predicted_probability), 0)\n", "            #all_timePerFrame_host = all_timePerFrame_host + [time_per_image]\n", "            # print(\"testing\")\n", "    print('copying some data back to cpu for generating confusion matrix...')\n", "    y_true = all_labels_d.cpu()\n", "    y_predicted = all_predictions_d.cpu()  # to('cpu')\n", "    testset_predicted_probabilites = all_predictions_probabilities_d.cpu()  # to('cpu')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    #return y_predicted, testset_predicted_probabilites, all_timePerFrame_host"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    cm = confusion_matrix(y_true, y_predicted)  # confusion matrix"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    print('Accuracy of the network on the %d test images: %f %%' % (total, (\n", "            100.0 * correct / total)))\n", "    print(cm)\n", "    print(\"taking class names to plot CM\")\n", "    class_names = test_dataloader.dataset.classes #test_datasets.classes  # taking class names for plotting confusion matrix\n", "    print(\"Generating confution matrix\")\n", "    plot_confusion_matrix(cm, classes=class_names, title='my confusion matrix')\n", "    \n\n", "    ##################################################################\n", "    # classification report\n", "    #################################################################\n", "    print(classification_report(y_true, y_predicted, target_names=class_names))\n\n", "    ##################################################################\n", "    # Standard metrics for medico Task\n", "    #################################################################\n", "    print(\"Printing standard metric for medico task\")\n", "    print(\"Accuracy =\",mtc.accuracy_score(y_true, y_predicted))\n", "    print(\"Precision score =\", mtc.precision_score(y_true,y_predicted, average=\"weighted\"))\n", "    print(\"Recall score =\", mtc.recall_score(y_true, y_predicted, average=\"weighted\"))\n", "    print(\"F1 score =\", mtc.f1_score(y_true, y_predicted, average=\"weighted\"))\n", "    print(\"Specificity =\")\n", "    print(\"MCC =\", mtc.matthews_corrcoef(y_true, y_predicted))\n\n", "    ##################################################################\n", "    # Standard metrics for medico Task\n", "    #################################################################\n", "    print(\"Printing standard metric for medico task\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    print(\"1. Recall score (REC) =\", mtc.recall_score(y_true, y_predicted, average=\"weighted\"))\n", "    print(\"2. Precision score (PREC) =\",\n", "            mtc.precision_score(y_true, y_predicted, average=\"weighted\"))\n", "    print(\"3. Specificity (SPEC) =\")\n", "    # print(\"4. Accuracy (ACC) =\", mtc.accuracy_score(y_true, y_predicted, weights))\n", "    print(\"5. Matthews correlation coefficient(MCC) =\", mtc.matthews_corrcoef(y_true, y_predicted))\n", "    print(\"6. F1 score (F1) =\", mtc.f1_score(y_true, y_predicted, average=\"weighted\"))\n", "    \n", "    print('Finished.. ')\n\n", "    #====================================================================\n", "    # Writing to a file\n", "    #=====================================================================\n", "    \n", "    np.set_printoptions(linewidth=np.inf)\n", "    with open(\"%s/%s_evaluation.csv\" % (opt.out_dir, py_file_name), \"w\") as f:\n", "        f.write(np.array2string(mtc.confusion_matrix(y_true, y_predicted), separator=\", \"))\n", "        f.write(\"\\n\\n\\n\\n\")\n", "        f.write(\"--- Macro Averaged Resutls ---\\n\")\n", "        f.write(\"Precision: %s\\n\" % mtc.precision_score(y_true, y_predicted, average=\"macro\"))\n", "        f.write(\"Recall: %s\\n\" % mtc.recall_score(y_true, y_predicted, average=\"macro\"))\n", "        f.write(\"F1-Score: %s\\n\\n\" % mtc.f1_score(y_true, y_predicted, average=\"macro\"))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        f.write(\"--- Micro Averaged Resutls ---\\n\")\n", "        f.write(\"Precision: %s\\n\" % mtc.precision_score(y_true, y_predicted, average=\"micro\"))\n", "        f.write(\"Recall: %s\\n\" % mtc.recall_score(y_true, y_predicted, average=\"micro\"))\n", "        f.write(\"F1-Score: %s\\n\\n\" % mtc.f1_score(y_true, y_predicted, average=\"micro\"))\n", "        f.write(\"--- Other Resutls ---\\n\")\n", "        f.write(\"MCC: %s\\n\" % mtc.matthews_corrcoef(y_true, y_predicted))\n", "    f.close()\n", "    print(\"Report generated\")\n\n", "    #=========================================================================="]}, {"cell_type": "markdown", "metadata": {}, "source": ["=============================================<br>\n", "Prepare submission file with probabilities<br>\n", "=============================================="]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def prepare_prediction_file():\n", "    if opt.bs != 1:\n", "        print(\"Please run with bs = 1\")\n", "        exit()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    test_model_checkpoint = opt.test_checkpoint #input(\"Please enter the path of test model:\")\n", "    checkpoint = torch.load(test_model_checkpoint)\n", "    model = prepare_model()\n", "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n", "    model.eval()\n", "    dataloaders = prepare_data()\n", "    test_dataloader = dataloaders[\"val\"]\n", "    class_names = test_dataloader.dataset.classes\n", "    df = pd.DataFrame(columns=[\"filename\", \"predicted-label\", \"actual-label\"] + class_names)\n", "    print(df.head())\n", "   #  exit()\n", "    with torch.no_grad():\n", "        for i, data in tqdm(enumerate(test_dataloader, 0)):\n", "            \n", "            inputs, labels, paths = data\n", "                \n", "            df_temp = pd.DataFrame(columns=[\"filename\", \"predicted-label\", \"actual-label\"] + class_names)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["            #print(\"paths:\", paths)\n", "            filename = [list(paths)[0].split(\"/\")[-1]]\n", "            #print(\"filenames:\", filename)\n", "            \n", "            df_temp[\"filename\"] = filename\n", "           \n", "            inputs = inputs.to(device)\n", "            labels = labels.to(device)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["            outputs = model(inputs)\n", "            outputs = F.softmax(outputs, 1)\n", "            predicted_probability, predicted = torch.max(outputs.data, 1)\n", "            \n", "            df_temp[\"predicted-label\"] = class_names[predicted.item()]\n", "            df_temp[\"actual-label\"] = class_names[labels.item()]\n", "            \n\n", "            # print(\"actual label:\", labels.item())\n", "            #print(\"predicted label:\", predicted.item())\n", "            # print(\"probabilities :\", outputs.cpu())\n", "            probabilities = outputs.cpu().squeeze()\n", "            probabilities = probabilities.tolist()\n", "            probabilities = np.around(probabilities, decimals=3)\n", "            #print(probabilities)\n", "            df_temp[class_names] = probabilities\n\n", "            #record = record + [class_names[labels.item()]] + [class_names[predicted.item()]] \n\n", "            #print(record)\n", "            #print(df_temp.head())\n", "            df = df.append(df_temp)\n", "           # break\n", "        print(df.head())\n", "        print(\"length of DF:\", len(df))\n", "        prob_file_name = \"%s/%s_probabilities.csv\" % (opt.out_dir, py_file_name)\n", "        df.to_csv(prob_file_name, index=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["########################################################<br>\n", "Prepare submission file:<br>\n", "########################################################"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def prepare_submission_file(image_names, predicted_labels, max_probability, time_per_image, submit_dir, data_classes):\n", "    predicted_label_names = []\n", "    for i in predicted_labels:\n", "        predicted_label_names = predicted_label_names + [data_classes[i]]\n\n", "    #  print(predicted_label_names)\n", "    submission_dataframe = pd.DataFrame(np.column_stack([image_names,\n", "                                                            predicted_label_names,\n", "                                                            max_probability,\n", "                                                            time_per_image]),\n", "                                    columns=['images', 'labels', 'PROB', 'time'])\n", "    #print(\"image names:{0}\".format(image_names))\n", "    submission_dataframe.to_csv(os.path.join(submit_dir, \"method_3_test_output\"), index=False)\n", "    print(submission_dataframe)\n", "    print(\"successfully created submission file\")\n", "###########################################################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#########################################################<br>\n", " Ploting history and save plots to plots directory<br>\n", "#########################################################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##########################################################<br>\n", "Plot confusion matrix - method<br>\n", "##########################################################"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def plot_confusion_matrix(cm, classes,\n", "                            normalize=False,\n", "                            title='Confusion matrix',\n", "                            cmap=plt.cm.Blues,\n", "                            plt_size=[10,10]):\n", "    \"\"\"\n", "    This function prints and plots the confusion matrix.\n", "    Normalization can be applied by setting `normalize=True`.\n", "    \"\"\"\n", "    plt.rcParams['figure.figsize'] = plt_size\n", "    if normalize:\n", "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n", "        print(\"Normalized confusion matrix\")\n", "    else:\n", "        print('Confusion matrix, without normalization')\n", "    print(cm)\n", "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n", "    plt.title(title)\n", "    plt.colorbar()\n", "    tick_marks = np.arange(len(classes))\n", "    plt.xticks(tick_marks, classes, rotation=90)\n", "    plt.yticks(tick_marks, classes)\n", "    fmt = '.2f' if normalize else 'd'\n", "    thresh = cm.max() / 2.\n", "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n", "        plt.text(j, i, format(cm[i, j], fmt),\n", "                    horizontalalignment=\"center\",\n", "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n", "    plt.tight_layout()\n", "    plt.ylabel('True label')\n", "    plt.xlabel('Predicted label')\n", "    # plt.savefig(os.path.join(plot_dir, cm_plot_name))\n", "    figure = plt.gcf()\n", "    writer.add_figure(\"Confusion Matrix\", figure)\n", "    print(\"Finished confusion matrix drawing...\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == '__main__':\n", "    print(\"Started data preparation\")\n", "    data_loaders = prepare_data()\n", "    print(vars(opt))\n", "    print(\"Data is ready\")\n\n", "    # Train or retrain or inference\n", "    if opt.action == \"train\":\n", "        print(\"Training process is strted..!\")\n", "        run_train()\n", "       # pass\n", "    elif opt.action == \"retrain\":\n", "        print(\"Retrainning process is strted..!\")\n", "        run_train(retrain=True)\n", "       # pass\n", "    elif opt.action == \"test\":\n", "        print(\"Inference process is strted..!\")\n", "        test_model()\n", "    elif opt.action == \"check\":\n", "        check_model_graph()\n", "        print(\"Check pass\")\n", "    elif opt.action == \"prepare\":\n", "        prepare_prediction_file()\n", "        print(\"Probability file prepared..!\")\n\n", "    # Finish tensorboard writer\n", "    writer.close()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}